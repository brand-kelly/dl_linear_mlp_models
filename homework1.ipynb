{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import csv\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_NAMES = ['background', 'kart', 'pickup', 'nitro', 'bomb', 'projectile']\n",
    "\n",
    "\n",
    "class SuperTuxDataset(Dataset):\n",
    "    def __init__(self, dataset_path):\n",
    "        \"\"\"\n",
    "        Your code here\n",
    "        Hint: Use the python csv library to parse labels.csv\n",
    "\n",
    "        WARNING: Do not perform data normalization here. \n",
    "        \"\"\"\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.transform = transforms.ToTensor()\n",
    "        self.complte_path = dataset_path + \"/labels.csv\"\n",
    "        with open(self.complte_path) as csv_file:\n",
    "            reader = csv.reader(csv_file, delimiter=\",\")\n",
    "\n",
    "            next(reader)\n",
    "\n",
    "            for row in reader:\n",
    "                self.images.append(dataset_path + \"/\" + row[0])\n",
    "                self.labels.append(LABEL_NAMES.index(row[1]))\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Your code here\n",
    "        \"\"\"\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Your code here\n",
    "        return a tuple: img, label\n",
    "        \"\"\"\n",
    "        image = Image.open(self.images[idx])\n",
    "        image_tensor = self.transform(image)\n",
    "        return image_tensor, self.labels[idx]\n",
    "\n",
    "\n",
    "def load_data(dataset_path, num_workers=0, batch_size=128):\n",
    "    dataset = SuperTuxDataset(dataset_path)\n",
    "    return DataLoader(dataset, num_workers=num_workers, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "\n",
    "\n",
    "def accuracy(outputs, labels):\n",
    "    outputs_idx = outputs.max(1)[1].type_as(labels)\n",
    "    return outputs_idx.eq(labels).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = load_data(\"./data/train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inputs, labels in train_loader:\n",
    "    inputs_evaluation = inputs\n",
    "    labels_evaluation = labels\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Class Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$-\\log\\left(\\frac{\\exp(x_l) }{ \\sum_j \\exp(x_j)} \\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationLoss(torch.nn.Module):\n",
    "    def forward(self, input, target):\n",
    "        \"\"\"\n",
    "        Your code here\n",
    "\n",
    "        Compute mean(-log(softmax(input)_label))\n",
    "\n",
    "        @input:  torch.Tensor((B,C))\n",
    "        @target: torch.Tensor((B,), dtype=torch.int64)\n",
    "\n",
    "        @return:  torch.Tensor((,))\n",
    "\n",
    "        Hint: Don't be too fancy, this is a one-liner\n",
    "        \"\"\"\n",
    "        mean_loss = F.cross_entropy(input, target)\n",
    "        return mean_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        Your code here\n",
    "        \"\"\"\n",
    "        self.linear = nn.Linear(in_features=3*64*64, out_features=6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Your code here\n",
    "\n",
    "        @x: torch.Tensor((B,3,64,64))\n",
    "        @return: torch.Tensor((B,6))\n",
    "        \"\"\"\n",
    "        x = x.view(-1, 3*64*64)\n",
    "        return self.linear(x)\n",
    "\n",
    "\n",
    "class MLPClassifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        Your code here\n",
    "        \"\"\"\n",
    "        self.sequential = nn.Sequential(\n",
    "            nn.Linear(in_features = 3*64*64, out_features = 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features = 256, out_features = 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features = 128, out_features = 6)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Your code here\n",
    "\n",
    "        @x: torch.Tensor((B,3,64,64))\n",
    "        @return: torch.Tensor((B,6))\n",
    "        \"\"\"\n",
    "        x = x.view(-1, 3*64*64)\n",
    "        x = self.sequential(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_factory = {\n",
    "    'linear': LinearClassifier,\n",
    "    'mlp': MLPClassifier,\n",
    "}\n",
    "\n",
    "\n",
    "def save_model(model):\n",
    "    from torch import save\n",
    "    from os import path\n",
    "    for n, m in model_factory.items():\n",
    "        if isinstance(model, m):\n",
    "            return save(model.state_dict(), path.join(path.dirname(path.abspath(__file__)), '%s.th' % n))\n",
    "    raise ValueError(\"model type '%s' not supported!\" % str(type(model)))\n",
    "\n",
    "\n",
    "def load_model(model):\n",
    "    from torch import load\n",
    "    from os import path\n",
    "    r = model_factory[model]()\n",
    "    r.load_state_dict(load(path.join(path.dirname(path.abspath(__file__)), '%s.th' % model), map_location='cpu'))\n",
    "    return r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = MLPClassifier()\n",
    "criterion = ClassificationLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model_type='linear', epochs=10):\n",
    "    model = model_factory[model_type]()\n",
    "    train_loader = load_data(\"./data/train\")\n",
    "    val_loader = load_data(\"./data/valid\")\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "    criterion = ClassificationLoss()\n",
    "    writer = SummaryWriter(log_dir=f'runs/{model_type}_train')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        count = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            count += 1\n",
    "            if count % 50 == 0:\n",
    "                writer.add_scalar('Training/Loss', total_loss / count, (epoch + 1) * count)\n",
    "                print(f'[Training] Epoch: {epoch + 1}/{epochs} - Count: {count}: {total_loss/count}')\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_samples = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            total_loss = 0\n",
    "            count = 0\n",
    "            for inputs, labels in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                count += 1\n",
    "                if val_samples % 100 == 0:\n",
    "                    writer.add_scalar('Validation/Loss', total_loss / count, (epoch + 1) * count)\n",
    "                    print(f'[Validation] Epoch: {epoch + 1}/{epochs} - Count: {count}: {total_loss/count}')\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10 - Count: 50: 1.7043973517417907\n",
      "Epoch: 1/10 - Count: 100: 1.596347371339798\n",
      "Epoch: 1/10 - Count: 150: 1.4944430581728618\n",
      "Epoch: 2/10 - Count: 50: 1.109833184480667\n",
      "Epoch: 2/10 - Count: 100: 1.0563062697649002\n",
      "Epoch: 2/10 - Count: 150: 1.0112765292326609\n",
      "Epoch: 3/10 - Count: 50: 0.8452679479122162\n",
      "Epoch: 3/10 - Count: 100: 0.839406788945198\n",
      "Epoch: 3/10 - Count: 150: 0.8235836776097616\n",
      "Epoch: 4/10 - Count: 50: 0.7628085958957672\n",
      "Epoch: 4/10 - Count: 100: 0.7526772117614746\n",
      "Epoch: 4/10 - Count: 150: 0.7318068087100983\n",
      "Epoch: 5/10 - Count: 50: 0.6954132652282715\n",
      "Epoch: 5/10 - Count: 100: 0.6752057605981827\n",
      "Epoch: 5/10 - Count: 150: 0.6637463609377543\n",
      "Epoch: 6/10 - Count: 50: 0.6298600697517395\n",
      "Epoch: 6/10 - Count: 100: 0.625765155851841\n",
      "Epoch: 6/10 - Count: 150: 0.6163016454378764\n",
      "Epoch: 7/10 - Count: 50: 0.5857199186086655\n",
      "Epoch: 7/10 - Count: 100: 0.5740009704232216\n",
      "Epoch: 7/10 - Count: 150: 0.5733136488993963\n",
      "Epoch: 8/10 - Count: 50: 0.5618437474966049\n",
      "Epoch: 8/10 - Count: 100: 0.5543137490749359\n",
      "Epoch: 8/10 - Count: 150: 0.5421966751416524\n",
      "Epoch: 9/10 - Count: 50: 0.49672294974327086\n",
      "Epoch: 9/10 - Count: 100: 0.5128229025006295\n",
      "Epoch: 9/10 - Count: 150: 0.513159906466802\n",
      "Epoch: 10/10 - Count: 50: 0.49313628613948823\n",
      "Epoch: 10/10 - Count: 100: 0.4876090928912163\n",
      "Epoch: 10/10 - Count: 150: 0.483137615720431\n"
     ]
    }
   ],
   "source": [
    "train('mlp', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_data(args):\n",
    "    dataset = SuperTuxDataset(args.dataset)\n",
    "\n",
    "    f, axes = plt.subplots(args.n, len(LABEL_NAMES))\n",
    "\n",
    "    counts = [0]*len(LABEL_NAMES)\n",
    "\n",
    "    for img, label in dataset:\n",
    "        c = counts[label]\n",
    "        if c < args.n:\n",
    "            ax = axes[c][label]\n",
    "            ax.imshow(img.permute(1, 2, 0).numpy())\n",
    "            ax.axis('off')\n",
    "            ax.set_title(LABEL_NAMES[label])\n",
    "            counts[label] += 1\n",
    "        if sum(counts) >= args.n * len(LABEL_NAMES):\n",
    "            break\n",
    "\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
